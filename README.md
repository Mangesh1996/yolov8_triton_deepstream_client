# yolov8-triton-deepstream-client
```
change the input file in the run_docker.sh file.(support format  RTSP and MP4)
```

## Demo
Run demo:
```bash
bash run_docker.sh 
```
## NOTE
```
This is the only Yolov7-deeptstream-client inference on GRPC, you also need a Triton server to load the Yolov7 plan file (engine) model.
kindly refer.:-
```
<details><summary> <b>Expand</b> </summary>

* (https://github.com/marcoslucianops/DeepStream-Yolo.git)

* (https://github.com/triton-inference-server/server)
</details>



## Acknowledgements

<details><summary> <b>Expand</b> </summary>

* [https://github.com/marcoslucianops/DeepStream-Yolo.git](https://github.com/marcoslucianops/DeepStream-Yolo.git)
* [https://github.com/NVIDIA-AI-IOT/deepstream_python_apps](https://github.com/NVIDIA-AI-IOT/deepstream_python_apps)

</details>
